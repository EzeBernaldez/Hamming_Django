ResÃºmen de Fundamentos de la Inteligencia Artificial


A modo de resÃºmen, ademÃ¡s de los apuntes que tenemos de clases y de los resÃºmenes que vayamos haciendo, realizaremos los ejercicios teÃ³ricos de los prÃ¡cticos.


TeorÃ­a 2 - IntroducciÃ³n a los Agentes Inteligentes

Un agente inteligente es un sistema situado en un ambiente al que percibe
mediante sus sensores y sobre el que actÃºa mediante sus
actuadores/efectores [Russell y Norvig]


Otra definiciÃ³n un poco mÃ¡s precisa serÃ­a:
Un agente inteligente es un sistema (o entidad) fÃ­sico o virtual, situado en algÃºn ambiente, que es capaz de actuar de manera autÃ³noma y flexible en este ambiente a los fines de lograr los objetivos que le han sido delegadosâ€ [Wooldridge, 2009]

Aspectos de un agente

UbicaciÃ³n en un ambiente (â€œsituatednessâ€):
El comportamiento del agente es afectado fuertemente por el ambiente en el cual estÃ¡ embebido:
El agente percibe directamente el ambiente mediante sus sensores. 
El agente actÃºa mediante sus efectores modificando el ambiente y sus percepciones futuras.

Corporalidad ( embodiment): caso particular de â€œsituatednessâ€ donde el agente tiene un cuerpo fÃ­sico que restringe su interacciÃ³n con el ambiente.

El ambiente podrÃ­a ser todo, Â¡el universo entero! En la prÃ¡ctica, es sÃ³lo esa parte del universo cuyo estado nos importa al diseÃ±ar este agente, la parte que afecta a lo que el agente percibe y que se ve afectada por las acciones del agente.

AutonomÃ­a:
El agente debe ser capaz de actuar sin la intervenciÃ³n directa de humanos (u otros agentes).
Tiene control sobre su propio estado interno (sÃ­mil a objetos).
Tiene control sobre su propias acciones (no existe una relaciÃ³n master/slave).
Puede, si es necesario, modificar su comportamiento en base a la experiencia (aprender).

Flexibilidad:
Reactivo: el agente percibe su ambiente y responde en un tiempo adecuado a los cambios que se producen en Ã©l.
Pro-activo: el agente no sÃ³lo actÃºa en respuesta al ambiente sino que puede tomar la iniciativa (comportamiento dirigido por el objetivo).
Social: capacidad para interactuar, cuando es apropiado, con otros agentes artificiales o humanos.


Aspectos de un agente

Podemos pensar a los agentes como funciones, es decir una descripciÃ³n matemÃ¡tica abstracta:
Dados
Un conjunto de estados del ambiente
S = {s0, s1, s2, . . . , s|S|}.
Un conjunto de acciones A (capacidades efectoras del agente).

Un agente puede ser considerado en forma abstracta como una funciÃ³n
Ag : S âˆ— â†’ A donde S âˆ— es el conjunto de secuencias de estados en S.







Refinando la visiÃ³n quedaria algo asi:


Un conjunto de observaciones (o percepciones) posibles

O = {o0, o1, o2, . . . , o|O|}.
Una funciÃ³n de percepciÃ³n P : S â†’ O.
Una funciÃ³n de acciÃ³n o comportamiento C : Oâˆ— â†’ A.







En el caso de la aspiradora quedarÃ­a de la siguiente manera:

FunciÃ³n de percepciÃ³n P: El agente percibe en quÃ© casilla estÃ¡ y si existe suciedad en la misma.
Conjunto de observaciones: O = {[A, Limpio], [A, Sucio], [B, Limpio], [B, Sucio]}

Un comportamiento C muy simple: â€œsi la casilla actual estÃ¡ sucia entonces aspirar, sino moverse a la otra casillaâ€.






Estados del ambiente y transiciÃ³n entre estados

El ambiente, de acuerdo a la transiciÃ³n entre estados serÃ¡ considerado:

DeterminÃ­stico: el siguiente estado del entorno estÃ¡ determinado por el estado actual y la acciÃ³n ejecutada por el/los agente/s. Es decir que sÃ© con claridad a quÃ© estado voy.
T : S Ã— A â†’ S

No determinÃ­stico: No puedo saber con claridad a quÃ© estado irÃ­a desde el estado actual, podrÃ­a ir a ningÃºn estado o a varios estados a la vez.

T : S Ã— A â†’ 2^S o bien T : S Ã— A â†’ Pr(S)


Agente Racional

Lo que es racional en un momento dado depende de cuatro cosas: 
La medida de rendimiento que define el criterio de Ã©xito.
El conocimiento previo del entorno por parte del agente.
Las acciones que puede realizar el agente.
La secuencia de percepciÃ³n del agente hasta el momento. 

Es decir, la racionalidad de un agente depende de la medida de performance, su conocimiento previo del ambiente, y los sensores y efectores que tenga.

Esto conduce a una definiciÃ³n de un agente racional:
â€œPara cada secuencia de percepciones posibles, un agente racional deberÃ­a poder seleccionar aquella acciÃ³n que se espera que maximice su medida de performance, dada la evidencia provista por la secuencia de percepciones y cualquier conocimiento previo que el agente tenga.â€

Tenemos que tener cuidado de distinguir entre racionalidad y omnisciencia. Un agente omnisciente conoce el resultado real de sus acciones y puede actuar en consecuencia; Pero la omnisciencia es imposible en la realidad, por ejemplo si cruzo la calle cuando percibo que no viene ningÃºn auto, pero me cae una puerta de un aviÃ³n antes de cruzar. (el ejemplo del profe en la clase)
Este ejemplo muestra que la racionalidad no es lo mismo que la perfecciÃ³n. La racionalidad maximiza el rendimiento esperado, mientras que la perfecciÃ³n maximiza el rendimiento real.

Nuestra definiciÃ³n de racionalidad no requiere, entonces, omnisciencia, porque la elecciÃ³n racional depende sÃ³lo de la secuencia perceptiva hasta la fecha. TambiÃ©n debemos asegurarnos de que no hemos permitido inadvertidamente que el agente se involucre en actividades decididamente poco inteligentes. Por ejemplo, si un agente no mira a ambos lados antes de cruzar una carretera muy transitada, entonces su secuencia de percepciÃ³n no le dirÃ¡ que hay un camiÃ³n grande que se acerca a alta velocidad. 

Realizar acciones con el fin de modificar preceptos futuros, a veces llamado recopilaciÃ³n de informaciÃ³n, es una parte importante de la racionalidad. En el ejemplo anterior no fue racional mirar ambos lados antes de cruzar, ya que era una secuencia de percepciones poco informativa.

Nuestra definiciÃ³n requiere que un agente racional no sÃ³lo reÃºna informaciÃ³n, sino que tambiÃ©n aprenda lo mÃ¡ximo posible de lo que percibe.

Por lo tanto Racional implica recolectar informaciÃ³n y aprendizaje.


Medida de performance

EvalÃºa cuÃ¡n deseable es la secuencia de estados del ambiente generados por la secuencia de acciones del agente. Es una medida cuantificable.
Si la secuencia es deseable, entonces el agente ha funcionado bien. Esta nociÃ³n de deseabilidad se captura mediante una medida de rendimiento.

Como regla general, es mejor diseÃ±ar medidas de rendimiento de acuerdo con lo que realmente se quiere lograr en el entorno, en lugar de segÃºn cÃ³mo se cree que deberÃ­a comportarse el agente.


DescripciÃ³n PAES

El primer paso para diseÃ±ar un agente racional es especificar el entorno de trabajo (problema o tarea a resolver), esto son Medida de Performance,el Ambiente, los Efectores, los Sensores.

Ejemplo: Taxi


Ejemplo: Compra en internet



IMPORTANTE: Cuando el agente no es un software, los efectores son las partes o dispositivos del agente, por ejemplo el robot del prÃ¡ctico en donde los efectores eran los brazos por ejemplo, si el agente es de software los efectores serÃ­an las acciones, por ejemplo el agente recomendador el efecto serÃ­a mostrar una pelÃ­cula.


Propiedades del Ambiente

Totalmente Observable vs. Parcialmente Observable: Si los sensores de un agente le dan acceso al estado completo del entorno en cada punto en el tiempo, entonces decimos que el entorno de la tarea es totalmente observable. Un entorno de tareas es efectivamente totalmente observable si los sensores detectan todos los aspectos que son relevantes para la elecciÃ³n de la acciÃ³n; la relevancia, a su vez, depende de la medida de rendimiento. Los entornos totalmente observables son convenientes porque el agente no necesita mantener ningÃºn estado interno para realizar un seguimiento del mundo. Un entorno puede ser parcialmente observable debido a sensores ruidosos e imprecisos o porque simplemente faltan partes del estado en los datos del sensor, por ejemplo, un taxi automatizado no puede ver lo que otros conductores estÃ¡n pensando. Si el agente no tiene ningÃºn sensor, entonces el entorno es inobservable. 

Agente Ãºnico vs. Multiagente: La distinciÃ³n entre entornos de agente Ãºnico y multiagente puede parecer bastante simple. Por ejemplo, un agente que resuelve un crucigrama por sÃ­ solo estÃ¡ claramente en un entorno de un solo agente, mientras que un agente que juega al ajedrez estÃ¡ en un entorno de dos agentes. Sin embargo, hay algunos problemas sutiles. En primer lugar, hemos descrito cÃ³mo una entidad puede ser vista como un agente, pero no hemos explicado quÃ© entidades deben ser vistas como agentes. Â¿Un agente A (el taxista, por ejemplo) tiene que tratar a un objeto B (otro vehÃ­culo) como un agente, o puede ser tratado simplemente como un objeto? 
La distinciÃ³n clave es si el comportamiento de B se describe mejor como la maximizaciÃ³n de una medida de rendimiento cuyo valor depende del comportamiento del agente A.
Por ejemplo, en el ajedrez, la entidad oponente B estÃ¡ tratando de maximizar su mediciÃ³n de rendimiento, lo que, segÃºn las reglas del ajedrez, minimiza la medida de rendimiento del agente A. Por lo tanto, el ajedrez es un entorno multiagente. Por otro lado, en el entorno de la conducciÃ³n de taxis, la prevenciÃ³n de colisiones maximiza la medida de rendimiento de todos los agentes, por lo que se trata de un entorno multiagente parcialmente cooperativo. 

Determinista vs. No Determinista: Si el siguiente estado del entorno estÃ¡ completamente determinado por el estado actual y la acciÃ³n ejecutada por el agente o los agentes, entonces decimos que el entorno es determinista; de lo contrario, no es determinista. En principio, un agente no necesita preocuparse por la incertidumbre en un entorno determinista totalmente observable. Sin embargo, si el entorno es parcialmente observable, entonces podrÃ­a parecer que no es determinista. La mayorÃ­a de las situaciones reales son tan complejas que es imposible hacer un seguimiento de todos los aspectos no observados; A efectos prÃ¡cticos, deben tratarse como no deterministas. La conducciÃ³n de taxis es claramente no determinista en este sentido, porque nunca se puede predecir con exactitud el comportamiento del trÃ¡fico; AdemÃ¡s, los neumÃ¡ticos pueden reventar inesperadamente y el motor puede atascarse sin previo aviso. 
La palabra estocÃ¡stico es utilizada por algunos como sinÃ³nimo de "no determinista", estocÃ¡stico, pero hacemos una distinciÃ³n entre los dos tÃ©rminos; decimos que un modelo del medio ambiente es estocÃ¡stico si se ocupa explÃ­citamente de las probabilidades.


EpisÃ³dico vs. Secuencial: En un entorno de tareas episÃ³dicas, la experiencia del agente se divide en episodios atÃ³micos. En cada episodio, el agente recibe una percepciÃ³n y luego realiza una sola acciÃ³n. Crucialmente, el prÃ³ximo episodio no depende de las acciones tomadas en los episodios anteriores. Muchas tareas de clasificaciÃ³n son episÃ³dicas. Por ejemplo, un agente que tiene que detectar piezas defectuosas en una lÃ­nea de montaje basa cada decisiÃ³n en la pieza actual, independientemente de las decisiones anteriores; AdemÃ¡s, la decisiÃ³n actual no afecta si la siguiente pieza es defectuosa. En entornos secuenciales, por otro lado, la decisiÃ³n actual podrÃ­a afectar a todas las decisiones futuras. El ajedrez y la conducciÃ³n de taxis son secuenciales: en ambos casos, las acciones a corto plazo pueden tener consecuencias a largo plazo. Los entornos episÃ³dicos son mucho mÃ¡s sencillos que los entornos secuenciales porque el agente no necesita pensar en el futuro.

EstÃ¡tico vs. DinÃ¡mico: Si el entorno puede cambiar mientras un agente estÃ¡ deliberando, entonces decimos que el entorno es dinÃ¡mico para ese agente; de lo contrario, es estÃ¡tico. Los entornos estÃ¡ticos son fÃ¡ciles de manejar porque el agente no necesita estar mirando el mundo mientras decide una acciÃ³n, ni necesita preocuparse por el paso del tiempo. Los entornos dinÃ¡micos, por otro lado, estÃ¡n continuamente preguntando al agente quÃ© quiere hacer; si aÃºn no se ha decidido, eso cuenta como decidir no hacer nada. Si el entorno en sÃ­ no cambia con el paso del tiempo, pero la puntuaciÃ³n de rendimiento del agente sÃ­ lo hace, entonces decimos que el entorno es semidinÃ¡mico. La conducciÃ³n de taxis es claramente dinÃ¡mica: los otros coches y el propio taxi siguen moviÃ©ndose mientras el algoritmo de conducciÃ³n vacila sobre quÃ© hacer a continuaciÃ³n. El ajedrez, cuando se juega con un reloj, es semidinÃ¡mico. Los crucigramas son estÃ¡ticos.

Discreto vs. Continuo: La distinciÃ³n discreto/continuo se aplica al estado del entorno, a la forma en que se maneja el tiempo y a las percepciones y acciones del agente. Por ejemplo, el entorno de ajedrez tiene un nÃºmero finito de estados distintos (excluyendo el reloj). El ajedrez tambiÃ©n tiene un conjunto discreto de percepciones y acciones. La conducciÃ³n de taxis es un problema de estado continuo y tiempo continuo: la velocidad y la ubicaciÃ³n del taxi y de los otros vehÃ­culos atraviesan un rango de valores continuos y lo hacen sin problemas a lo largo del tiempo. Las acciones de conducciÃ³n del taxi tambiÃ©n son continuas (Ã¡ngulos de direcciÃ³n, etc.). La entrada de las cÃ¡maras digitales es discreta, estrictamente hablando, pero normalmente se trata como si representara intensidades y ubicaciones que varÃ­an continuamente.
IMPORTANTE:Si la medida de performance intervienen magnitudes como velocidad, tiempo, peso, etc, es continuo

Conocido vs. Desconocido: Estrictamente hablando, esta distinciÃ³n no se refiere al entorno en sÃ­, sino al estado de conocimiento de las dinÃ¡micas de ese ambiente. En un entorno conocido, se dan los resultados (o las probabilidades de resultados si el entorno no es determinista) para todas las acciones. Obviamente, si el entorno es desconocido, el agente tendrÃ¡ que aprender cÃ³mo funciona para poder tomar buenas decisiones. La distinciÃ³n entre entornos conocidos y desconocidos no es la misma que entre entornos total y parcialmente observables. Es muy posible que un entorno conocido sea parcialmente observable, por ejemplo, en los juegos de cartas de solitario, conozco las reglas, pero sigo siendo incapaz de ver las cartas que aÃºn no se han volteado. Por el contrario, un entorno desconocido puede ser completamente observable: en un videojuego nuevo, la pantalla puede mostrar todo el estado del juego, pero todavÃ­a no sÃ© quÃ© hacen los botones hasta que los pruebo.

Algunos ejemplos



Estructura de Agente

Hasta ahora hemos hablado de los agentes describiendo el comportamiento, es decir, la acciÃ³n que se realiza despuÃ©s de una secuencia dada de percepciones. El trabajo de la IA es diseÃ±ar un programa de agente que implemente la funciÃ³n del agente: el programa del agente es el mapeo de percepciones a acciones. Suponemos que este programa se ejecutarÃ¡ en algÃºn tipo de dispositivo informÃ¡tico con sensores y actuadores fÃ­sicos, lo que llamamos la arquitectura del agente:
Agente = arquitectura + programa. 

Obviamente, el programa que elijamos tiene que ser uno que sea apropiado para la arquitectura. Si el programa va a recomendar acciones como Caminar, es mejor que la arquitectura tenga patas. La arquitectura puede ser una PC ordinaria o un automÃ³vil robÃ³tico con varias computadoras integradas, cÃ¡maras y otros sensores. 

En general, la arquitectura hace que las percepciones de los sensores estÃ©n disponibles para el programa, ejecuta el programa y alimenta las opciones de acciÃ³n del programa a los actuadores a medida que se generan.


Clases generales de programas de agentes

Los cuatro tipos bÃ¡sicos de programas de agentes que incorporan los principios que subyacen a casi todos los sistemas inteligentes son:

Agentes reflejos (o reactivos) simples
Agentes reflejos basados en modelo
Agentes basados en objetivos
Agentes basados en utilidades

Estos tipos de agentes pueden a su vez ser implementados como agentes de aprendizaje.



Agentes reflejos simples (ARS)

TambiÃ©n llamados agentes reactivos puros o agentes tropÃ­sticos. Seleccionan una acciÃ³n en base a la percepciÃ³n actual, ignorando el resto de la historia perceptual (el pasado). No existe internamente ninguna representaciÃ³n de estado.

Los agentes reflejos simples son la forma de agente mÃ¡s sencilla que basa sus acciones en la percepciÃ³n actual. Este agente no tiene memoria ni interactÃºa con otros agentes si le falta informaciÃ³n. Estos agentes funcionan con arreglo a un conjunto de los llamados reflejos o reglas, condiciÃ³n-acciÃ³n (o situaciÃ³n-acciÃ³n). Esto significa que el agente estÃ¡ preprogramado para realizar acciones que corresponden al cumplimiento de determinadas condiciones.
Si el agente se encuentra con una situaciÃ³n para la que no estÃ¡ preparado, no puede responder adecuadamente. Los agentes sÃ³lo son eficaces en entornos totalmente observables que permitan acceder a toda la informaciÃ³n necesaria.
Ejemplo:
si auto-adelante-estÃ¡-frenando entonces comenzar-a-frenar

Su comportamiento es dirigido por el principio de estÃ­mulo-respuesta caracterÃ­stico de los reflejos de humanos, animales y plantas.

Ventajas
Simplicidad.
Tiempo de respuesta mÃ­nimo.
Reglas simples pueden producir comportamientos colectivos complejos.
ImplementaciÃ³n directa en hardware (bueno para robÃ³tica).

Limitaciones: Los agentes reflejos simples tienen la admirable propiedad de ser simples, pero son de inteligencia limitada. SÃ³lo funcionarÃ¡ si se puede tomar la decisiÃ³n correcta sobre la base de la percepciÃ³n actual, es decir, sÃ³lo si el entorno es plenamente observable.

SÃ³lo trabajan bien si la acciÃ³n correcta puede determinarse en base a la percepciÃ³n actual. 
Posibilidad de loops infinitos bajo observabilidad parcial (por ejemplo la aspiradora, que sucede si no tiene el sensor de ubicaciÃ³n).
Incapacidad de analizar la consecuencia futura de las acciones.

Incluso un poco de inobservabilidad puede causar serios problemas. Por ejemplo, la regla de frenado supone que la condiciÃ³n en la que el coche de delante estÃ¡ frenando se puede determinar a partir de la percepciÃ³n actual: un solo fotograma de vÃ­deo. Esto funciona si el coche de delante tiene una luz de freno montada en el centro (y, por lo tanto, identificable de forma Ãºnica). Desafortunadamente, los modelos mÃ¡s antiguos tienen diferentes configuraciones de luces traseras, luces de freno y luces de seÃ±al de giro, y no siempre es posible saber a partir de una sola imagen si el automÃ³vil estÃ¡ frenando o simplemente tiene las luces traseras encendidas. Un simple agente de reflejos que condujera detrÃ¡s de un automÃ³vil de este tipo frenarÃ­a continua e innecesariamente o, peor aÃºn, nunca frenarÃ­a en absoluto.

El algoritmo genÃ©rico serÃ­a este:




Agentes reflejos basados en modelos (ARBM)

La forma mÃ¡s eficaz de manejar la observabilidad parcial es que el agente realice un seguimiento de la parte del mundo que no puede ver ahora. Es decir, el agente debe mantener algÃºn tipo de estado interno que dependa de la historia de percepciÃ³n y, por lo tanto, refleje al menos algunos de los aspectos del estado interno no observados del estado actual. 

Por ejemplo, para el problema de frenado, el estado interno no es demasiado extenso, solo el fotograma anterior de la cÃ¡mara, lo que permite al agente detectar cuÃ¡ndo dos luces rojas en el borde del vehÃ­culo se encienden o apagan simultÃ¡neamente. Para otras tareas de conducciÃ³n, como cambiar de carril, el agente debe realizar un seguimiento de dÃ³nde estÃ¡n los otros autos si no puede verlos todos a la vez. 

La actualizaciÃ³n de esta informaciÃ³n de estado interno, a medida que pasa el tiempo, requiere que dos tipos de conocimientos se codifiquen en el programa del agente de alguna forma.

En primer lugar, necesitamos informaciÃ³n sobre cÃ³mo cambia el mundo a lo largo del tiempo, que se puede dividir en dos partes: 
Los efectos de las acciones del agente
CÃ³mo evoluciona el mundo independientemente del agente. 

Por ejemplo, cuando el agente gira el volante en el sentido de las agujas del reloj, el coche gira a la derecha, y cuando llueve las cÃ¡maras del coche pueden mojarse. 
Este conocimiento acerca de "cÃ³mo funciona el mundo", ya sea implementado en circuitos booleanos simples o en teorÃ­as cientÃ­ficas completas, se denomina modelo de transiciÃ³n del mundo.

En segundo lugar, necesitamos alguna informaciÃ³n sobre cÃ³mo se refleja el estado del mundo en las percepciones del agente. Por ejemplo, cuando el coche de delante inicia la frenada, aparecen una o varias regiones rojas ilustradas en la imagen de la cÃ¡mara orientada hacia delante y, cuando la cÃ¡mara se moja, aparecen objetos en forma de gota en la imagen que oscurecen parcialmente la carretera. Este tipo de conocimiento se denomina modelo de sensor.

Juntos, el modelo de transiciÃ³n y el modelo de sensor permiten a un agente realizar un seguimiento del estado del mundo, en la medida de lo posible dadas las limitaciones de los sensores del agente. Un agente que utiliza estos modelos se denomina agente basado en modelos.

El estado interno permite solucionar o aliviar los problemas de observabilidad parcial.

Si bien toma en cuenta el pasado no considera el futuro (no planifica).






Agentes basados en objetivos (ABO)

Saber algo sobre el estado actual del medio ambiente no siempre es suficiente para decidir quÃ© hacer. Por ejemplo, en un cruce de carreteras, el taxi puede girar a la izquierda, girar a la derecha o seguir recto. La decisiÃ³n correcta depende de dÃ³nde intente llegar el taxi. En otras palabras, ademÃ¡s de una descripciÃ³n del estado actual, el agente necesita algÃºn tipo de informaciÃ³n de objetivo que describa situaciones que son deseables, por ejemplo, estar en un destino determinado. El programa del agente puede combinar esto con el modelo (la misma informaciÃ³n que se utilizÃ³ en el agente reflejo basado en modelo) para elegir acciones que logren el objetivo. 

En la selecciÃ³n de acciones se toma en cuenta informaciÃ³n sobre los objetivos (estados deseables) a alcanzar. El logro de un objetivo puede requerir analizar las consecuencias futuras de secuencias completas de acciones (planes).

A veces, la selecciÃ³n de acciones basada en objetivos es sencilla, por ejemplo, cuando la selecciÃ³n de objetivos es inmediata de una sola acciÃ³n. A veces serÃ¡ mÃ¡s complicado, por ejemplo, cuando el agente tiene que considerar largas secuencias de giros y vueltas para encontrar una manera de lograr el objetivo. La bÃºsqueda y la planificaciÃ³n son los subcampos de la IA dedicados a encontrar secuencias de acciones que logren los objetivos del agente.

La toma de decisiones de este tipo es fundamentalmente diferente de las reglas de acciÃ³n condicional descritas anteriormente, en el sentido de que implica la consideraciÃ³n del futuro. En los diseÃ±os de agentes reflejos, esta informaciÃ³n no se representa explÃ­citamente, ya que las reglas integradas se asignan directamente de las percepciones a las acciones. El agente reflejo frena cuando ve las luces de freno, punto. No tiene ni idea de por quÃ©. Un agente basado en objetivos frena cuando ve las luces de freno porque esa es la Ãºnica acciÃ³n que predice que lograrÃ¡ su objetivo de no golpear a otros autos.


Agentes basados en utilidades (ABU)

Los objetivos por sÃ­ solos no son suficientes para generar un comportamiento de alta calidad en la mayorÃ­a de los entornos. Por ejemplo, muchas secuencias de acciones llevarÃ¡n al taxi a su destino (logrando asÃ­ el objetivo), pero algunas son mÃ¡s rÃ¡pidas, seguras, mÃ¡s fiables o mÃ¡s baratas que otras. Las metas sÃ³lo proporcionan una distinciÃ³n binaria cruda entre estados "felices" e "infelices". Una medida de rendimiento mÃ¡s general deberÃ­a permitir una comparaciÃ³n de diferentes estados del mundo de acuerdo con exactamente lo feliz que harÃ­an al agente. Debido a que "feliz" no suena muy cientÃ­fico, los economistas y los cientÃ­ficos de la computaciÃ³n usan el tÃ©rmino utilidad en su lugar. 
La funciÃ³n de utilidad de un agente es esencialmente una internalizaciÃ³n de la medida de rendimiento. Siempre que la funciÃ³n de utilidad interna y la medida de rendimiento externa estÃ©n de acuerdo, un agente que elija acciones para maximizar su utilidad serÃ¡ racional de acuerdo con la medida de rendimiento externa. 
Cuentan con una funciÃ³n de utilidad:
U : S â†’ R
que captura las preferencias del agente por los estados del mundo.

Al igual que los agentes basados en objetivos, un agente basado en la utilidad tiene muchas ventajas en tÃ©rminos de flexibilidad y aprendizaje. AdemÃ¡s, en dos tipos de casos, los objetivos son inadecuados, pero un agente basado en la utilidad aÃºn puede tomar decisiones racionales. 
En primer lugar, cuando hay objetivos contradictorios, de los cuales solo se pueden alcanzar algunos (por ejemplo, la velocidad y la seguridad), la funciÃ³n de utilidad especifica la compensaciÃ³n adecuada. 
En segundo lugar, cuando hay varios objetivos que el agente puede aspirar, ninguno de los cuales se puede lograr con certeza, la utilidad proporciona una forma en la que la probabilidad de Ã©xito se puede calcular con la importancia de los objetivos.

La observabilidad parcial y el no determinismo son omnipresentes en el mundo real, por lo que la toma de decisiones se toma en condiciones de incertidumbre. TÃ©cnicamente hablando, un agente racional basado en la utilidad elige la acciÃ³n que maximiza la utilidad esperada de los resultados de la acciÃ³n, es decir, la utilidad esperada que el agente espera derivar, en promedio, dadas las probabilidades y utilidades de cada resultado.

Un agente que posee una funciÃ³n de utilidad explÃ­cita puede tomar decisiones racionales con un algoritmo de propÃ³sito general que no depende de la funciÃ³n de utilidad especÃ­fica que se estÃ¡ maximizando. De esta manera, la definiciÃ³n "global" de racionalidad, que designa como racionales aquellas funciones de agente que tienen el rendimiento mÃ¡s alto, se convierte en una restricciÃ³n "local" a los diseÃ±os de agentes racionales que pueden expresarse en un programa simple.

Un agente ABU sigue los principios de teorÃ­a de decisiÃ³n para balancear la deseabilidad (utilidad) de los resultados con la probabilidad de que Ã©stos ocurran.

El principio de la utilidad esperada mÃ¡xima (UEM) especifica que un agente racional debe seleccionar aquella acciÃ³n que maximice su utilidad esperada.






Agentes Conversacionales y Tipos de Agentes


TeorÃ­a 3 - ResoluciÃ³n de problemas y BÃºsqueda No Informada


Agentes de resoluciÃ³n de problemas

En esta unidad se abordarÃ¡ el agente basado en objetivo denominado Agente de resoluciÃ³n de problemas, el cual es un agente que considera acciones futuras y la deseabilidad de sus resultados. Estos encuentran secuencias de acciones que conducen a estados deseables u objetivos para lograr el objetivo, es decir el conjunto de estados del mundo que satisfacen alguna condiciÃ³n de deseabilidad.

Estos agentes establecen un proceso de bÃºsqueda, el cual buscan una secuencia de acciones que conduce a estados de valor conocido y eligen la mejor (trata de encontrar la mejor acciÃ³n o secuencia de acciones para llegar a un objetivo). El agente sÃ³lo tiene un conjunto de acciones inmediatas y, el algoritmo de bÃºsqueda tiene como entrada un problema y como salida la soluciÃ³n o indicaciÃ³n de falla. Una soluciÃ³n (o plan) es una secuencia de acciones que conducen a un estado objetivo desde el estado actual. La fase de ejecuciÃ³n del plan es aquella en que las acciones recomendadas o establecidas en la soluciÃ³n son llevadas a cabo.

Para lograr su objetivo, los agentes siguen los pasos de formular (formular el objetivo y el problema), buscar (generar sucesores de estados ya explorados) y ejecutar la soluciÃ³n.


FormulaciÃ³n del problema

Para formular un problema de estado unico se definen formalmente cinco componentes:
El estado inicial.
Las acciones posibles en cada estado.
Una descripciÃ³n de lo que hacen las acciones (modelo de transiciÃ³n), mediante operadores, y una funciÃ³n sucesor o funciÃ³n Result(s, a) que genera un nuevo nodo hijo o sucesor para cada uno de los estados resultantes.
El test de objetivo.
La funciÃ³n de costo del paso (que mide la calidad de una soluciÃ³n para obtener una soluciÃ³n Ã³ptima, mientras menor costo de paso mejor).

Por lo general, las acciones posibles y su descripciÃ³n conforman un solo componente, por lo que se los consideran 4 componentes.


BÃºsqueda de soluciones

El objetivo principal es generar sucesores de estados ya explorados expandiendo estados mediante la aplicaciÃ³n de acciones vÃ¡lidas en dicho estado, luego elegir un estado a explorar, controlar si el estado es un estado objetivo y expandir el estado en base a una estrategia de bÃºsqueda. 

Una estrategia de bÃºsqueda es un criterio que determina cuÃ¡l es el prÃ³ximo estado a expandir. Se define especificando el orden de expansiÃ³n de los nodos (un nodo estÃ¡ compuesto del estado, el nodo padre, la acciÃ³n y el costo de paso) implementando una frontera (puede ser una cola FIFO, LIFO o prioridades que se ordena de forma tal para expandir el primer nodo de la cola), el cual es la colecciÃ³n de nodos que estÃ¡n esperando para ser expandidos. Las estrategias son evaluadas de acuerdo a las siguientes dimensiones: completitud (siempre encuentra una soluciÃ³n si alguna existe?), complejidad de tiempo (cuanto tarda en encontrar una soluciÃ³n?), complejidad de espacio (Â¿cuÃ¡nta memoria se necesita?) y optimalidad (siempre encuentra la mejor soluciÃ³n (la de menor costo)?).

Para la estrategia de bÃºsqueda se consideran estructuras de datos, las cuales son el Ã¡rbol, que permite estados repetidos, y el grafo, que detecta los estados repetidos y no los incluye dentro de la estructura, manteniendo un conjunto de nodos explorados de manera que los nodos no se repitan, es decir que soluciona el problema de loops dentro del algoritmo (evita problemas exponenciales). El Ã¡rbol de bÃºsqueda es la estructura resultante de la aplicaciÃ³n del proceso de bÃºsqueda, y cada nodo corresponde a un estado en el espacio de estados y las aristas del nodo corresponden a acciones. La raÃ­z del Ã¡rbol o grafo es el estado inicial del problema. Es importante aclarar que Ã¡rbol/grafo es distinto de espacio de estados, el Ãºltimo describe el conjunto de estados del mundo y sus acciones, y el otro describe rutas entre estados hacia un objetivo.





Estrategias de bÃºsqueda

La complejidad es medida en tÃ©rminos de: 
bâ€”factor de ramificaciÃ³n mÃ¡ximo del Ã¡rbol de bÃºsqueda, es decir, el nÃºmero mÃ¡ximo de hijos que puede tener un nodo.
dâ€”profundidad del nodo objetivo menos profundo.
mâ€”profundidad mÃ¡xima del espacio de estado (longitud mÃ¡x. de
cualquier paso en el espacio de estados).
El tiempo es medido en tÃ©rminos del nÃºmero de nodos generados en
la bÃºsqueda y el espacio en tÃ©rminos del mÃ¡ximo nÃºmero de nodos
almacenados en la memoria.


Estrategias de bÃºsqueda no informadas

Def. Son estrategias que sÃ³lo usan la informaciÃ³n disponible en la definiciÃ³n del problema. Estas se clasifican:
BÃºsqueda primero en anchura (BPA).
BÃºsqueda costo uniforme (BCU).
BÃºsqueda primero en profundidad (BPP).
BÃºsqueda en profundidad limitada (BPL).
BÃºsqueda con profundidad iterativa (BPI).

BÃºsqueda primero en anchura: Expande el nodo no expandido menos profundo. El test de objetivo es realizado cuando el nodo es generado (antes de colocarlo en la frontera).

La frontera es una cola FIFO: los nuevos sucesores van al final.

Esta bÃºsqueda es completa si el factor de ramificaciÃ³n mÃ¡ximo del Ã¡rbol de bÃºsqueda es finito. 
El tiempo es b + b^2 + â€¦ + b^d = O(b^d) Exponencial en d.
El espacio es O(b^d) (mantiene cada nodo en memoria cuando es un grafo; en Ã¡rbol podrÃ­a generar menos pasos pero al tener redundancias el tiempo serÃ¡ problema).
Es Ã³ptima con costos de paso idÃ©nticos; no en caso general.

El espacio es el gran problema.

BÃºsqueda de costo uniforme: Es la extensiÃ³n Ã³ptima de la BPA con costos de paso distintos. Trata de expandir el nodo que tiene menos costo de paso g(n) total, desde el nodo raÃ­z al actual. Realiza el test de objetivo cuando el nodo es seleccionado para expansiÃ³n (cuando lo saca de la frontera). 

La frontera es una cola ordenada por costo de paso (cola por prioridad), el mÃ¡s bajo primero.

Esta bÃºsqueda es completa si y sÃ³lo sÃ­ el costo de cada paso es mayor igual al costo mÃ­nimo entre todos los arcos del Ã¡rbol/grafo (e). El costo mÃ­nimo debe ser mayor a 0, lo cual garantiza que el costo acumulado crece con cada paso, ademÃ¡s de que obliga a la estrategia a llegar a los nodos que tienen la soluciÃ³n ya que asÃ­ evitamos los bucles de bajo o cero costo.
El tiempo coincide con el de BPA cuando los costos de paso son idÃ©nticos. En general es O(b^âŒˆCâˆ—â€‹/eâŒ‰), donde C* es el costo de la soluciÃ³n Ã³ptima y cada acciÃ³n cuesta >= e.
En espacio es igual que el BPA.
Es Ã³ptima ya que los nodos son expandidos incrementalmente en g(n).

El tiempo puede ser mucho mayor que el de BPA porque explora grandes Ã¡rboles con pequeÃ±os pasos. En el mejor de los casos es b^(d+1) cuando los costos son iguales.

BÃºsqueda primero en profundidad: Expande el nodo no expandido mÃ¡s profundo. Realiza el test de objetivo cuando el nodo es seleccionado para expansiÃ³n (cuando lo saca de la frontera).

La frontera es una cola LIFO, es decir, coloca sucesores al frente.

No es completa ya que falla en espacios de profundidad infinita (espacios con loops). SÃ­ lo es cuando hay espacios finitos en bÃºsqueda en grafo.
El tiempo es O(b^m). Es malo si m es mucho mÃ¡s grande que d (pero caso con bÃºsqueda en Ã¡rbol).
En bÃºsqueda en Ã¡rbol, la complejidad espacial de DFS es O(bm), porque solo se necesita mantener el camino actual desde la raÃ­z hasta el nodo en expansiÃ³n.
En bÃºsqueda en grafo, si se usa una estructura para detectar ciclos (como un conjunto de visitados), la complejidad espacial puede crecer hasta ser igual al tiempo, O(b^m), ya que se deben almacenar todos los nodos generados.
No es Ã³ptima.

BÃºsqueda en profundidad limitada: Es bÃºsqueda primero en profundidad pero incorpora un lÃ­mite de profundidad L, es decir, nodos a profundidad L no tienen sucesores.

Es completa si L es mayor o igual a d (profundidad de nodo objetivo menos profundo)
El tiempo es O(b^L).
En espacio es O(bL). Si fuese en grafo y se guardan los nodos visitados, podrÃ­a ser O(b^L).
No es Ã³ptima si el nodo objetivo es mÃ¡s profundo que L.

BÃºsqueda en profundidad iterativa: Es igual que bÃºsqueda en profundidad limitada con la diferencia que es aplicada iterativamente, incrementando L.

Es completa.
El tiempo es O(b^d)= (d+1)b^0 + db1 + (d-1)b^2 + â€¦ + b^d.
En espacio, es O(bd).
Es Ã³ptima con costo de paso = 1.

Es la estrategia de bÃºsqueda no informada preferida cuando el espacio de bÃºsqueda es grande y la profundidad de la soluciÃ³n es desconocida.


6. Â¿QuÃ© es una heurÃ­stica?

La funciÃ³n heurÃ­stica es denotada como h(n) que significa coste estimado del camino mÃ¡s barato desde el nodo n hasta el nodo objetivo.

7. Â¿QuÃ© diferencia fundamental existe entre la bÃºsqueda no informada y la bÃºsqueda informada?

La diferencia fundamental que existe entre la bÃºsqueda no informada y la bÃºsqueda informada es que la Ãºltima necesita de datos relacionados con el problema a tratar, es decir que dentro de la estrategia de bÃºsqueda utiliza otras medidas y funciones. Estas funciones se denominan heurÃ­sticas, donde estiman costos de los caminos mÃ¡s cortos.


8. Â¿Bajo quÃ© condiciones se dice que una funciÃ³n heurÃ­stica h es admisible? Â¿Y consistente?

Una heurÄ±stica es admisible cuando nunca sobreestima el costo real de alcanzar
el objetivo
Aâˆ— usa una heurÃ­stica admisible
es decir, h(n) â‰¤ hâˆ—(n) donde hâˆ—(n) es el costo verdadero desde n
(TambiÃ©n requiere que h(n) â‰¥ 0, asÃ­ h(G) = 0 para cualquier objetivo G)

Una heurÃ­stica es consistente si para cada nodo n y cada sucesor nâ€² de n generado
por la acciÃ³n a, el costo estimado de llegar al objetivo desde n no es mayor que
el costo del paso de llegar a nâ€² mÃ¡s el costo estimado de llegar al objetivo desde
nâ€², h(n) â‰¤ c(n, a, nâ€²) + h(nâ€²)




Tipo de bÃºsqueda
Completa
Optima
BPL
Profundidad limitada. No informada.
Si, lÃ­mite >= profundidad del nodo objetivo menos profundo.
No.
BPI
Profundidad iterativa. No informada.
SÃ­.
SÃ­, con costo de paso igual a 1.
BPMV
BÃºsqueda primero el mejor voraz. Informada
No, puede caer en loops. Completa en espacio finito con chequeo de estado repetido
No, puede repetir pasos.
BPP
BÃºsqueda primero en profundidad. No informada
No, falla en espacios de profundidad infinita. Completa en espacios finitos.
No.
BA*
BÃºsqueda A*. Informada.
Completa, a menos que haya infinitos nodos con f <= f(G)
SÃ­, no puede expandir fi+1 hasta que fi haya finalizado.
BCU
BÃºsqueda de costo uniforme. No informada.
SÃ­, sÃ³lo si el costo de cada paso es >= a Îµ.
SÃ­.
BPA
BÃºsqueda en anchura. No informada.
SÃ­, si b es finito.
SÃ­, con costo de paso idÃ©nticos; no, en el caso general.


3) Dado el problema 8-puzzle con estado inicial como se muestra en la Figura 1(a) y el estado objetivo referido en la Figura 1(b), se pide:



1. En la teorÃ­a 4, se presenta al nÃºmero de fichas fuera de lugar como una heurÃ­stica admisible para el 8-puzzle.

a) Usando esta heurÃ­stica, ejecute manualmente el algoritmo PM Voraz basado en BÃºsqueda- Ãrbol, considerando los estados inicial y objetivo de las Figuras 1(a) y 1(b), respectivamente. En cada expansiÃ³n que realice, en el caso de ser aplicables, disponga las acciones en el orden siguiente: derecha, abajo, izquierda, arriba. Indique en cada nodo
del Ã¡rbol resultante, el orden en que el mismo fue expandido.



b) Usando esta heurÃ­stica, ejecute manualmente el algoritmo Aâˆ— basado en BÃºsqueda- Ãrbol, considerando los estados inicial y objetivo de las Figuras 1(a) y 1(b), respectivamente. Recuerde mantener el orden de expansiÃ³n de las acciones propuesto anteriormente, como asÃ­ tambiÃ©n indicar en cada nodo del Ã¡rbol resultante, el orden en que el mismo fue expandido. En el caso que en la frontera del Ã¡rbol, hubieran dos o mÃ¡s nodos, cuyo valor de funciÃ³n de evaluaciÃ³n fuera el mismo, se deberÃ¡ expandir primero el de mÃ¡s a la izquierda.


c) Comente quÃ© diferencias habrÃ­a en los Ã¡rboles de bÃºsqueda obtenidos, si en lugar de
BÃºsqueda-Ãrbol se hubiera utilizado BÃºsqueda-Grafo.


Practico 3


3) 1)
Variables={x1,x2,x3,x4}
Dominio= {1,2,3,4,5,6,7,8,9,0} Ã³ {0-9}
RestricciÃ³n= En una misma asignaciÃ³n de valores no se cumple que:
x1=x2=x3=x4

2) Unarias: no hay.
Binarias: que dos dÃ­gitos no se repitan.
Ternarias: que no hayan secuencias consecutivas ascendentes o descendentes de tres dÃ­gitos.

3) AsignaciÃ³n parcial.
SoluciÃ³n.
Completa.






TeorÃ­a 6 - Agentes LÃ³gicos


Agentes LÃ³gicos (basados en conocimiento)

Los Agentes LÃ³gicos son agentes que derivan sus acciones mediante el razonamiento lÃ³gico.

Tienen como componente principal una base de conocimiento (BC), el cual es un conjunto de sentencias en un lenguaje formal. Cada sentencia se expresa en un lenguaje llamado lenguaje de representaciÃ³n del conocimiento y representa alguna afirmaciÃ³n sobre el mundo. Cuando la sentencia se toma como dada sin derivarse de otras oraciones, la llamamos axioma. Un ingeniero de conocimiento construye la BC en una serie de pasos.

Debe haber una forma de agregar nuevas sentencias a la base de conocimientos y una forma de consultar lo que se sabe. Los nombres estÃ¡ndar para estas operaciones son DECIR y PREGUNTAR, respectivamente. Ambas operaciones pueden implicar inferencia, es decir, derivar nuevas sentencias a partir de las antiguas. La inferencia debe obedecer al requisito de que cuando uno hace una pregunta de la base de conocimiento, la respuesta debe seguirse de lo que se ha dicho a la base de conocimiento previamente, en el sentido de que el proceso de inferencia no deberÃ­a inventar las cosas a medida que avanza.



Cada vez que se llama al programa agente, hace tres cosas.
(DECIR) Le dice a la base de conocimientos lo que percibe.
(PREGUNTAR) Pregunta a la base de conocimientos quÃ© acciÃ³n debe realizar. En el proceso de responder a esta pregunta, se puede hacer un razonamiento extenso sobre el estado actual del mundo, sobre los resultados de las posibles secuencias de acciÃ³n, etc. 
(DECIR) El programa agente le dice a la base de conocimientos quÃ© acciÃ³n se eligiÃ³ y devuelve la acciÃ³n para que pueda ejecutarse.

CREAR-SENTENCIA-DE-PERCEPCIÃ“N construye una oraciÃ³n que afirma que el agente percibiÃ³ la percepciÃ³n dada en un momento dado. CREAR-CONSULTA-ACCIÃ“N construye una sentencia que pregunta quÃ© acciÃ³n se debe realizar en el momento actual. Finalmente, CREAR-SENTENCIA-ACCIÃ“N construye una sentencia que afirma que la acciÃ³n elegida fue ejecutada.

Un agente basado en el conocimiento se puede construir simplemente diciÃ©ndole lo que necesita saber. Partiendo de una base de conocimientos vacÃ­a, el ingeniero de conocimiento de agentes puede DECIR frases una por una hasta que el agente sepa cÃ³mo operar en su entorno. A esto se le llama el enfoque declarativo para la construcciÃ³n de sistemas.

Los agentes pueden ser visualizados a nivel del conocimiento i.e., lo que conocen, independiente de cÃ³mo estÃ¡ implementado, o a nivel de la implementaciÃ³n i.e., las estructuras en la BC y los algoritmos que las manipulan.

OntologÃ­as: concepto fundamental en agentes basados en conocimiento. Consisten del vocabulario (predicados, funciones y constantes) resultante al traducir conceptos importantes a nivel de dominio a sus respectivos nombres a nivel lÃ³gico.

El agente debe ser capaz de:
Representar estados, acciones, etc.
Incorporar nuevas percepciones.
Actualizar representaciones internas del mundo.
Deducir propiedades ocultas del mundo: por ejemplo si afuera de una casa estÃ¡ nevando y el agente estÃ¡ adentro, una propiedad oculta es que hace frÃ­o afuera, el agente debe ser capaz de deducir esa propiedad.
Deducir acciones apropiadas.


Mundo de Wumpus

El mundo Wumpus es una cueva que consta de habitaciones conectadas por pasadizos. Acechando en algÃºn lugar de la cueva estÃ¡ el terrible wumpus, una bestia que se come a cualquiera que entre en su habitaciÃ³n. El Wumpus puede ser disparado por un agente, pero el agente solo tiene una flecha. Algunas habitaciones contienen pozos sin fondo que atraparÃ¡n a cualquiera que deambule por estas habitaciones (excepto el wumpus, que es demasiado grande para caer). La Ãºnica caracterÃ­stica redentora de este entorno sombrÃ­o es la posibilidad de encontrar un montÃ³n de oro. 

DescripciÃ³n PAES
Medida de performance: oro 1000, muerte -1000, -1 por paso,-10 por usar la flecha.

Ambiente: Casillas adyacentes al Wumpus son olorosas, casillas adyacentes a un pozo tienen brisas, destello si el oro estÃ¡ en la misma casilla, disparar mata al Wumpus si estÃ¡ enfrente, disparar consume la Ãºnica flecha, agarrar alza el oro si estÃ¡ en la misma casilla, arrojar deja el oro en la misma casilla. El juego termina cuando el agente muere o cuando el agente sale de la cueva.

Efectores: girar Izquierda, girar Derecha, Adelante, Tomar, Arrojar, Disparar.

Sensores: Brisa, Destello, Olor (tambiÃ©n Grito y Golpe).

Las percepciones se entregarÃ¡n al programa agente en forma de una lista de cinco sÃ­mbolos; por ejemplo, si hay un hedor y una brisa, pero no hay destello, golpe o grito, el programa del agente obtendrÃ¡ [Olor,Brisa,Ninguno,Ninguno,Ninguno].


Podemos caracterizar el entorno del wumpus a lo largo de las diversas dimensiones Claramente, es determinista, discreto, estÃ¡tico y de un solo agente. (El wumpus no se mueve, afortunadamente.) Es secuencial, porque las recompensas pueden llegar sÃ³lo despuÃ©s de que se hayan realizado muchas acciones. Es parcialmente observable, porque algunos aspectos del estado no son directamente perceptibles: la ubicaciÃ³n del agente, el estado de salud del wumpus y la disponibilidad de una flecha. En cuanto a las ubicaciones de los pozos y los wumpus: podrÃ­amos tratarlos como partes no observadas del estado.

Para un agente del entorno, el principal reto es su ignorancia inicial de la configuraciÃ³n del entorno; superar esta ignorancia parece requerir un razonamiento lÃ³gico. En la mayorÃ­a de los casos del mundo wumpus, es posible que el agente recupere el oro de forma segura. De vez en cuando, el agente debe elegir entre volver a casa con las manos vacÃ­as o arriesgarse a morir para hacerse con el oro. 




RepresentaciÃ³n de Conocimiento

Conocimiento inicial sobre cÃ³mo â€œfunciona el mundoâ€, en el caso del Wumpus es:
UbicaciÃ³n y situaciÃ³n inicial del agente.
relaciÃ³n entre brisas y pozos,
relaciÃ³n entre olores y Wumpus,
Existencia de un (y solo un) Wumpus,
Conocimiento sobre las percepciones recibidas.
Conocimiento sobre cÃ³mo trabajan las acciones (modelo de transiciÃ³n).

Una BC en LP para el mundo de Wumpus

Arr, Ab, Der, Iz representan la orientaciÃ³n del agente.
Sea Pi,j true si hay un pozo en la casilla [i, j].
Sea Bi,j true si hay una brisa en la casilla [i, j].
Sea Ui,j true si el agente estÃ¡ ubicado en la casilla [i, j].
Sea Wi,j true si el Wumpus estÃ¡ en la casilla [i, j].
Sea Si,j true si hay olor en la casilla [i, j].

UbicaciÃ³n y situaciÃ³n inicial del agente:
R1: U1,1 R4: Â¬B1,1
R2: Der R5: Â¬W1,1
R3: Â¬P1,1 R6: Â¬S1,1


























La sentencia de percepciÃ³n serÃ­a:





























LPO = lÃ³gica de primer Ã³rden





















Inferencia / ImplicaciÃ³n LÃ³gica

BC âŠ¢i Î± significa que la sentencia Î± puede ser derivada desde BC mediante el procedimiento i.

Por ejemplo:
Las consecuencias de BC son un pajar; Î± es una aguja.
ImplicaciÃ³n lÃ³gica = aguja en un pajar;
Inferencia = encontrarla.

Sensatez (soundness): se dice que i es sensato (o sÃ³lido, o consistente o que mantiene la verdad) si siempre que BC âŠ¢i Î±, es tambiÃ©n verdadero que BC |= Î±.

Completitud: i es completa si siempre que BC |= Î±, es tambiÃ©n verdadero que BC âŠ¢i Î±

Recordando lÃ³gica:

BC |= Î± :se lee "BC satisface Î±" o "Î± es consecuencia semÃ¡ntica de BC" significa que Î± es verdadero en todos los modelos donde BC es verdadero.

BC âŠ¢ Î± : (se lee "BC prueba Î±" o "Î± es deducible de BC") significa que Î± puede ser deducido de BC usando un sistema de inferencia (reglas de deducciÃ³n, como modus ponens, etc.).

De manera mÃ¡s general:
|= Î± (se suele decir "Î± es vÃ¡lidamente verdadero" o "Î± es una tautologÃ­a") significa que Î± es verdadero en todos los modelos posibles, no depende de ningÃºn conjunto de premisas.
Es una verdad lÃ³gica: siempre es cierto, pase lo que pase.
âŠ¢ Î± significa que Î± puede ser probado en un sistema de inferencia sin ninguna premisa.
Es decir, hay una prueba formal de Î± usando solo reglas del sistema.


Recordando PROLOG






ClÃ¡usula de Horn

Una clÃ¡usula de Horn es una disyunciÃ³n de literales (variables o negaciones de variables) con a lo sumo un literal positivo.

Puede tener ningÃºn literal positivo (todos negados) o
Puede tener exactamente uno positivo (los demÃ¡s, si hay, son negaciones).


Formalmente, una clÃ¡usula de Horn es una fÃ³rmula de este tipo:
(Â¬A1â€‹âˆ¨Â¬A2â€‹âˆ¨â‹¯âˆ¨Â¬An â€‹âˆ¨ B)
donde todas las A son negadas y B es opcional y puede ser positiva o negada

La decisiÃ³n de la implicaciÃ³n con las clÃ¡usulas de Horn se puede hacer en un tiempo que es lineal en el tamaÃ±o de la base de conocimientos, una agradable sorpresa.

Por lo tanto podemos ver a la BC como una conjunciÃ³n de clÃ¡usulas de Horn. Estas van a ser usadas por los siguientes algoritmos.


Encadenamiento hacia atrÃ¡s

Es un mÃ©todo de razonamiento que parte del objetivo (lo que queremos probar) e intenta demostrarla buscando reglas que lleguen a ese objetivo.
La idea es trabajar hacia atrÃ¡s desde la consulta q, para probar q mediante BC:
Chequear si q ya es conocido, o
Probar usando la BC todas las premisas de alguna regla que concluye q.

Evitar loops: chequear si un nuevo subobjetivo ya estÃ¡ sobre el stack de objetivos.

Evitar trabajo repetido: chequear si un nuevo subobjetivo
ya se ha demostrado que es verdadero, o
ya ha fallado.

El encadenamiento hacia atrÃ¡s es dirigido por el objetivo (goal-driven), apropiado para resoluciÃ³n de problemas,e.g., Â¿CÃ³mo alcanzÃ³ un plan de doctorado?

La complejidad del encadenamiento hacia atrÃ¡s puede ser mucho menor que lineal en el tamaÃ±o de la BC.




Resumen del shhhhaaatt:
Objetivo: probar ğ‘„
Vamos hacia atrÃ¡s:
Â¿Para probar Q, quÃ© necesito?
P (porque ğ‘ƒâ‡’ğ‘„)
Â¿Para probar P, quÃ© necesito?
L âˆ§ M (porque ğ¿âˆ§ğ‘€â‡’ğ‘ƒ)
Â¿Para probar L y M?
Vamos uno por uno.
ğŸ”¹ Primero L:
Â¿CÃ³mo puedo obtener L?
O bien de ğ´âˆ§ğ‘ƒâ‡’ğ¿
o de ğ´âˆ§ğµâ‡’ğ¿
Tenemos que ver si alguna podemos activar:
A ya lo tenemos (hecho âœ…).
B tambiÃ©n (hecho âœ…).
Entonces usando ğ´âˆ§ğµâ‡’ğ¿ como tenemos A y B, podemos obtener L âœ…
ğŸ”¹ Ahora M:
Â¿CÃ³mo puedo obtener M?
De ğµâˆ§ğ¿â‡’ğ‘€
Â¿Tenemos B? âœ… (hecho)
Â¿Tenemos L? âœ… (reciÃ©n demostrado).
Entonces, aplicando ğµâˆ§ğ¿â‡’ğ‘€ podemos obtener M âœ…

Ahora tenemos L y M,por ğ¿âˆ§ğ‘€â‡’ğ‘ƒ
Lâˆ§Mâ‡’P, obtenemos P âœ…
Luego, por ğ‘ƒâ‡’ğ‘„ ,obtenemos Q âœ…

Encadenamiento hacia adelante (EA)

Idea: disparar cualquier regla cuyas premisas son satisfechas en la BC, agregar su conclusiÃ³n a la BC, hasta que la consulta es encontrada (es lo mismo que hacia atrÃ¡s pero empezamos desde los hechos).

El encadenamiento hacia adelante es dirigido por los datos (data-driven) y es comÃºn en procesamiento inconsciente, automÃ¡tico, e.g., reconocimiento de obje-
tos, decisiones de rutina.

Puede realizar una cantidad de trabajo irrelevante al objetivo.

ğŸ”¹ Estado inicial:
Hechos conocidos: {A, B}

ğŸ”¹ Â¿QuÃ© reglas puedo disparar?

Regla 5: ğ´âˆ§ğµâ‡’ğ¿

Tenemos A y B â†’ disparamos y obtenemos L. âœ…

Ahora hechos = {A, B, L}

ğŸ”¹ Â¿QuÃ© mÃ¡s puedo disparar?

Regla 3: ğµâˆ§ğ¿â‡’ğ‘€

Tenemos B y L â†’ disparamos y obtenemos M. âœ…

Ahora hechos = {A, B, L, M}

ğŸ”¹ Â¿QuÃ© mÃ¡s puedo disparar?

Regla 2: ğ¿âˆ§ğ‘€â‡’ğ‘ƒ

Tenemos L y M â†’ disparamos y obtenemos P. âœ…

Ahora hechos = {A, B, L, M, P}

ğŸ”¹ Â¿QuÃ© mÃ¡s puedo disparar?

Regla 1: ğ‘ƒâ‡’ğ‘„

Tenemos P â†’ disparamos y obtenemos Q. âœ…

Ahora hechos = {A, B, L, M, P, Q}

âœ… Â¡Listo! Â¡Llegamos a Q!



IngenierÃ­a de conocimiento

El proceso general de construcciÃ³n de la base de conocimiento, es llamado ingenierÃ­a del conocimiento. Un ingeniero del conocimiento es alguien que investiga el conocimiento en un dominio particular, aprende quÃ© conceptos son importantes en ese dominio y crea una representaciÃ³n formal de los objetos y relaciones en el dominio. Ilustramos el proceso de ingenierÃ­a del conocimiento en un dominio de circuitos electrÃ³nicos.

Los proyectos de ingenierÃ­a del conocimiento varÃ­an ampliamente en contenido, alcance y dificultad, pero todos estos proyectos incluyen los siguientes pasos: 

Identificar la tarea: quÃ© consultas harÃ© a la BC y quÃ© tipos de hechos tendrÃ© disponible.
El ingeniero del conocimiento debe delinear la gama de preguntas que la base de conocimientos admitirÃ¡ y los tipos de hechos que estarÃ¡n disponibles para cada instancia de problema especÃ­fico. Por ejemplo, Â¿la base de conocimientos de wumpus debe ser capaz de elegir acciones, o sÃ³lo es necesaria para responder a preguntas sobre el contenido del entorno? Â¿Los datos del sensor incluirÃ¡n la ubicaciÃ³n actual? La tarea determinarÃ¡ quÃ© conocimiento debe representarse para conectar las instancias del problema con las respuestas. Este paso es anÃ¡logo al proceso de PAES para el diseÃ±o de agentes.

AdquisiciÃ³n del conocimiento: Entender cÃ³mo trabaja el dominio.
Es posible que el ingeniero del conocimiento ya sea un experto en el dominio, o que necesite trabajar con expertos reales para extraer lo que saben, un proceso llamado adquisiciÃ³n de conocimiento. En esta etapa, el conocimiento no se representa formalmente. La idea es comprender el alcance de la base de conocimientos segÃºn lo determinado por la tarea, y comprender cÃ³mo funciona realmente el dominio. Para el mundo wumpus, que estÃ¡ definido por un conjunto artificial de reglas, el conocimiento relevante es fÃ¡cil de identificar. Para dominios reales, la cuestiÃ³n de la relevancia puede ser bastante difÃ­cil, por ejemplo, un sistema para simular diseÃ±os VLSI podrÃ­a o no necesitar tener en cuenta las capacitancias parÃ¡sitas y los efectos de la piel.

Decidir sobre el vocabulario de predicados, funciones y constantes: Involucra decisiones ontolÃ³gicas. Resultado: una ontologÃ­a (vocabulario).
Es decir, traducir los conceptos importantes de nivel de dominio en nombres de nivel lÃ³gico. Esto implica muchas cuestiones de estilo de ingenierÃ­a del conocimiento. Al igual que el estilo de programaciÃ³n, esto puede tener un impacto significativo en el Ã©xito final del proyecto. Por ejemplo, Â¿los pozos deben ser representados por objetos o por un predicado unario en cuadrados? Â¿La orientaciÃ³n del agente debe ser una funciÃ³n o un predicado? Â¿La ubicaciÃ³n del wumpus debe depender del tiempo? Una vez que se han hecho las elecciones, el resultado es un vocabulario que se conoce como ontologÃ­a del dominio. La palabra ontologÃ­a significa una teorÃ­a particular de la naturaleza del ser o de la existencia. La ontologÃ­a determina quÃ© tipos de cosas existen, pero no determina sus propiedades e interrelaciones especÃ­ficas.

Codificar conocimiento general sobre el dominio: El ingeniero del conocimiento escribe los axiomas de todos los tÃ©rminos del vocabulario. Esto precisa (en la medida de lo posible) el significado de los tÃ©rminos, lo que permite al experto verificar el contenido. A menudo, este paso revela conceptos errÃ³neos o lagunas en el vocabulario que deben corregirse volviendo al paso 3 e iterando a travÃ©s del proceso.

Codificar descripciÃ³n de la instancia del problema especÃ­fico: Si la ontologÃ­a estÃ¡ bien pensada, este paso es fÃ¡cil. Consiste en escribir oraciones atÃ³micas simples sobre instancias de conceptos que ya forman parte de la ontologÃ­a. Para un agente lÃ³gico, las instancias de problemas son suministradas por los sensores, mientras que una base de conocimiento "intangible" recibe oraciones de la misma manera que los programas tradicionales reciben datos de entrada.

Plantear consultas al procedimiento de inferencia y obtener respuestas: AquÃ­ es donde estÃ¡ la recompensa: podemos dejar que el procedimiento de inferencia opere sobre los axiomas y los hechos especÃ­ficos del problema para derivar los hechos que nos interesa conocer. Por lo tanto, evitamos la necesidad de escribir un algoritmo de soluciÃ³n especÃ­fico de la aplicaciÃ³n.

Depurar y evaluar la base de conocimientos. Por desgracia, las respuestas a las preguntas rara vez serÃ¡n correctas en el primer intento. MÃ¡s precisamente, las respuestas serÃ¡n correctas para la base de conocimientos tal como estÃ¡ escrita, suponiendo que el procedimiento de inferencia sea sÃ³lido, pero no serÃ¡n las que el usuario espera. Por ejemplo, si falta un axioma, algunas consultas no se podrÃ¡n responder desde la base de conocimientos. PodrÃ­a producirse un proceso de depuraciÃ³n considerable. Los axiomas faltantes o los axiomas que son demasiado dÃ©biles se pueden identificar fÃ¡cilmente al notar los lugares donde la cadena de razonamiento se detiene inesperadamente. 
Por ejemplo Si la BC incluye una regla diagnÃ³stica para encontrar al
Wumpus: âˆ€ s HayOlor(s) =â‡’ Adyacente(Home(Wumpus), s) en vez de un â‡â‡’ , el agente nunca serÃ¡ capaz de probar la ausencia de un Wumpus.

En la secciÃ³n 8.4.2 hay un ejemplo de cÃ³mo aplicar esto, no lo pongo porque el resumen se harÃ­a muy largo (mÃ¡s de los que ya estÃ¡)


TeorÃ­a 7 - OntologÃ­as y Grafos de Conocimiento
 

OntologÃ­as y Sistemas basados en Conocimiento

Para construir, de manera flexible, bases de conocimientos que pueden interoperar semÃ¡nticamente, hay estÃ¡ndares sobre cÃ³mo formalizar las relaciones: prop(Individuo, Propiedad, Valor), es la representaciÃ³n individuo-propiedad-valor o representaciÃ³n de tri-upla.

Cuando se tiene una sola relaciÃ³n, prop, Ã©sta puede ser omitida sin pÃ©rdida de informaciÃ³n:

Se puede formar, mediante esto, redes semÃ¡nticas, donde realizamos un grafo de conocimiento relacionando nodos iniciales (los cuales son los objetos o individuos) con nodos destino (los cuales son los valores) mediante transiciones rotuladas con la propiedad.

Turtle

Es un lenguaje utilizado para construir tri-uplas con una sintaxis entendible para la mÃ¡quina. Ã‰sta sintaxis estÃ¡ dada por: Sujeto Verbo Objeto.

Para agrupar objetos con el mismo sujeto y verbo, se utiliza la coma â€˜,â€™, es decir S V O1,O2 es una abreviatura de S V O1 y S V O2.

Para agrupar pares verbo-objeto para el mismo sujeto, se utiliza el punto y coma â€˜;â€™, es decir S V1 O1; V2 O2 es una abreviatura de S V1 O1 y S V2 O2.

Los corchetes se pueden usar para definir un individuo sin un identificador. Se puede usar como el objeto de una tri-upla.



Propiedades primitivas versus derivadas

Un conocimiento primitivo es el definido explÃ­citamente por hechos. Son hechos bÃ¡sicos que se almacenan directamente en la base de conocimientos. 

Un conocimiento derivado es el definido por reglas. Son hechos que no se almacenan directamente, sino que se calculan o infieren a partir de primitivas y reglas lÃ³gicas. Se obtienen mediante inferencia automÃ¡tica.

Una clase es un conjunto de individuos que son agrupados ya que tienen propiedades similares. Para definir una clase, asociamos propiedades a la misma, de modo que los individuos pueden ser una instancia de esa clase. Son caracterÃ­sticas calculadas o deducidas a partir de otras propiedades bÃ¡sicas mediante reglas o inferencia lÃ³gica.

Se permite, ademÃ¡s, una propiedad type entre el individuo y una clase. Se puede usar una propiedad especial subClassOf entre dos clases que permiten la herencia de la propiedad.

La lÃ³gica de una propiedad




Herencia mÃºltiple

Un individuo es usualmente un miembro de mÃ¡s de una clase, por lo que puede heredar las propiedades de todas las clases de las que es miembro. A su vez, hay que evitar que el individuo herede valores por defecto en conflicto desde las distintas clases, ya que tendrÃ­amos un problema de herencia mÃºltiple.


ElecciÃ³n de propiedades primitivas y derivadas

Se puede asociar un valor de propiedad con la clase mÃ¡s general con ese valor de propiedad. No se deben asociar propiedades ocasionales de una clase con la clase. Por ejemplo, si todas las computadoras actuales dan la casualidad que son marrones.







Represente en el formato universal (prop), mediante un programa Prolog, el siguiente conocimiento
Todas las computadoras Lemon tienen un Ã­cono de limÃ³n como logotipo y tienen color amarillo y verde.

prop(Obj, has_logo, lemon_icon) :- prop(Obj, type, lemon_computer).
prop(Obj, has_color, yellow) :- prop(Obj, type, lemon_computer).
prop(Obj, has_color, green) :- prop(Obj, type, lemon_computer).

Todas las laptops Lemon 10000 tienen un peso de 1,1 kg.

prop(Obj,weight_kg,1.1) :- prop(Obj,type,lemon_laptop_10000)

Las laptops Lemon 10000 son una subclase de las computadoras Lemon.

prop(Obj, subClassOf, lemon_computer) :- prop(Obj,type,lemon_laptop_10000)

La computadora comp_2347 es una laptop Lemon 10000

prop(comp_2347, type, lemon_laptop_10000).
Compruebe cÃ³mo el logo, los colores y el peso de comp_2347 pueden ser derivados de este programa. Â¿QuÃ© sucede con cualquier otra computadora del tipo laptop Lemon 10000 que sea incorporada en el futuro?

prop(lemon_laptop_10000, subClassOf, lemon_computer).
prop(Obj, type, lemon_computer) :- prop(Obj, type, lemon_laptop_10000), prop(lemon_laptop_10000, subClassOf, lemon_computer).
prop(comp_2347, type, lemon_laptop_10000).


En base al programa desarrollado en el Ejemplo 2 completo, obtenga una representaciÃ³n equivalente de todos los hechos del programa en el formato prop(Individuo,Propiedad,Valor) y grafique la red semÃ¡ntica correspondiente.

prop(donia_tota, madre, diego).
prop(don_diego, padre, diego).
prop(diego, padre, dalma).
prop(diego, profesiÃ³n, futbolista).
prop(ayrton, profesiÃ³n, automovilista).
prop(ayrton, nacimiento, brasil).
prop(lionel, nacimiento, argentina).
prop(diego, nacimiento, argentina).
prop(david, adopciÃ³n, francia).
prop(diego, es, campeÃ³n_mundial).
prop(ayrton, es, campeÃ³n_mundial).

prop(Obj, abuelo, Obj2) :- prop(Obj, padre, Obj1), prop(Obj1, padre, Obj2).
prop(Obj, abuelo, Obj2) :- prop(Obj, madre, Obj1), prop(Obj1, padre, Obj2).
prop(Obj, mÃ¡s_joven, Obj1) :- prop(Obj1, abuelo, Obj).
prop(Obj, trabaja, deportista) :- prop(Obj, profesiÃ³n, futbolista).
prop(Obj, trabaja, deportista) :- prop(Obj, profesiÃ³n, automovilista).
prop(Obj, nacionalidad, X) :- prop(Obj, nacimiento, X).
prop(Obj, nacionalidad, X) :- prop(Obj, adopciÃ³n, X).
prop(Obj, idolo, X) :- prop(Obj, nacionalidad, X), prop(Obj, trabaja, deportista), prop(Obj, es, campeÃ³n_mundial).







TeorÃ­a 8 - Grafos de Conocimiento, Web SemÃ¡ntica y los Grandes Modelos de Lenguaje (LLM)


Las computadoras sÃ³lo procesan sÃ­mbolos. Debido a esto debemos establecer mapping entre sÃ­mbolos, es decir conceptualizaciones donde realizamos correspondencias entre sÃ­mbolos usados en la computadora con los individuos y relaciones del mundo. Estas conceptualizaciones se hacen explÃ­citas mediante ontologÃ­as formales. 

A menudo, el conocimiento proviene de varias fuentes, por lo que debe ser integrado. Cada fuente tiene su propia terminologÃ­a y divide el mundo segÃºn necesidades (y evolucionan con el tiempo), por lo tanto los diseÃ±adores de KBS (Sistemas de Base de Conocimiento) deberÃ­an coincidir en cÃ³mo el mundo estÃ¡ dividido. Para ello es importante desarrollar un vocabulario comÃºn y un significado acordado para ese vocabulario, ademÃ¡s de que se deben desarrollar ontologÃ­as, es decir especificaciones de los significados de los sÃ­mbolos en un sistema de informaciÃ³n, para especificar quÃ© tipos de individuos son modelados, quÃ© propiedades son usadas y para dar axiomas que restringen el uso de este vocabulario.


OntologÃ­as, grafos de conocimiento y Web SemÃ¡ntica


Web SemÃ¡ntica

El objetivo era proveer soporte en la web para la navegaciÃ³n, visualizaciÃ³n de los sitios y establecer descripciones formales y semÃ¡ntica de los datos, permitiendo automatizar el acceso a estos datos y la interoperabilidad de los sistemas.

Comenzando los 2000, la W3C comienza a plantear iniciativas (estÃ¡ndares, protocolos, lenguajes, etc) tendientes a permitir que el conocimiento interpretable por la mÃ¡quina se distribuya en la World Wide Web. 

Se introdujeron conceptos claves como el RDF (Resource Description Framework), el cual es un estÃ¡ndar recomendado por la W3C para describir entidades o recursos, es decir que proporciona un modelo de datos estÃ¡ndar para un grafo de conocimiento. Un recurso puede ser cualquier cosa que podamos identificar, como una persona, una homepage, etc. En RDF, los recursos se describen con tri-uplas, tambiÃ©n conocidas como sentencias, y tienen un formato [sujeto predicado objeto]. Un grafo RDF consiste de un conjunto de tri-uplas:



TambiÃ©n, para serializar RDFs hay varias sintaxis para almacenar e intercambiar RDFs. Una de ellas son Turtle, RDF/XML, RDFa, N-Triples, JSON-LD, etc.


URIs e IRIs

Para referenciar a las entidades y relaciones en las tuplas necesitamos una identificaciÃ³n unÃ­voca de las mismas. 
Un URI (Uniform Resource Identifier) representa y direcciona los elementos fundamentales de los grafos de conocimiento, aunque tambiÃ©n suelen ser desreferenciables como URLs HTTP.
Un IRI (Internationalised Resource Identifier) tambiÃ©n son strings o cadenas que identifican unÃ­vocamente un recurso y son una generalizaciÃ³n de los URI (permiten una gama mÃ¡s amplia de caracteres).

Mediante estos URIs se realizan grafos que son traducidos por la mÃ¡quina para establecer relaciones (aquÃ­ se utilizan lenguajes para serializar, como turtle):




Lenguajes de OntologÃ­as

Para definir, en sÃ­mbolos, representaciones de individuos y relaciones necesitamos, ademÃ¡s de grafos de conocimiento, definiciones de taxonomÃ­as, es decir clases, subclases, propiedades y subpropiedades y restricciones que las clases deben cumplir.

Para definir taxonomÃ­as se requieren modelos de datos, aunque los lenguajes de ontologÃ­a/esquemas como RDFS y OWL son mÃ¡s potentes, ya que brindan estructuras de administraciÃ³n de conocimiento mÃ¡s completas y consistentes.

Las ontologÃ­as dan mÃ¡s dimensionalidad a un grafo de conocimiento permitiendo clasificar cosas y definir relaciones y atributos mÃ¡s especÃ­ficos. Las entidades pueden clasificarse especÃ­ficamente y convertirse en instancias de una o mÃ¡s clases. AdemÃ¡s, se pueden expresar restricciones para desarrollar un modelo de conocimiento mÃ¡s consistente. Con esto, se facilita la integraciÃ³n de datos, se controlan inconsistencias y se brinda informaciÃ³n semÃ¡ntica clave a los mecanismos de inferencia.


RDFS

Proporciona un lenguaje de esquema simple para RDF y permite declarar clases y propiedades, usando las clases predefinidas a nivel de lenguaje rdfs:Class y rdfs:Property. Las clases van con mayÃºsculas y las propiedades no.


OWL

Se basa en una familia de representaciones de conocimiento formal, llamadas lÃ³gicas de descripciÃ³n (LDs). Este lenguaje de ontologÃ­a/esquemas ha sido recomendado por la W3C como el estÃ¡ndar de hecho para ontologÃ­as de la Web. SintÃ¡cticamente se puede considerar como una extensiÃ³n de RDFS con vocabulario adicional, predefinido por el esquema OWL.

Este vocabulario de esquema sirve para construir ontologÃ­as y/o anotar sus datos, como restricciÃ³n de cardinalidad calificada, propiedad de cadena, autorrestricciÃ³n, etc.


Tipos de Grafos de Conocimiento (KG)

Grafos de Conocimiento del Mundo: no se centran en un Ãºnico campo de conocimiento, sino que intentan recopilar y estructurar todo el conocimiento del mundo. Representan conocimiento general, extensivo, impreciso y conectado.

Algunos ejemplos son Google Knowledge Graph, BFO, entre otros. Algunos se basan en los principios de Datos Abiertos Vinculados (Linked Open Data).

Grafos de Conocimiento del Dominio: grafos de conocimiento o al menos ontologÃ­as y taxonomÃ­as para los mÃ¡s variados campos de interÃ©s (negocios, salud, geogrÃ¡ficos, etc.). Representan conocimiento especÃ­fico, profundo, formal y con reglas claras.

Algunos ejemplos son LOV o BARTOC, y se pueden encontrar mÃ¡s fÃ¡cilmente con motores de bÃºsqueda especÃ­ficos.
Linked Open Data (LOD)

El objetivo es pasar de una Web de pÃ¡ginas (documentos) interconectadas (hipertextos) a una Web de Datos (datos vinculados) que conecta datos abiertos de distintas fuentes. Los datos abiertos estÃ¡n disponibles para su uso por cualquier aplicaciÃ³n, los cuales se crean usando estÃ¡ndares abiertos como RDF para describir metadatos de forma accesible y estructurada (es decir, los datos abiertos no solo requieren apertura legal como licencias, sino tambiÃ©n apertura tÃ©cnica, como formatos y estÃ¡ndares que permitan su libre uso e interpretaciÃ³n automÃ¡tica, para que las mÃ¡quinas los puedan entender).

Antes del 2006, la idea de vincular datos abiertos estaba restringida por la ausencia de una red conectada de datos en la web. Es por ello que DBpedia, un proyecto, comenzÃ³ extrayendo automÃ¡ticamente datos estructurados de Wikipedia, los convirtieron en tri-uplas RDF y, con cada cosa o entidad resultante, les otorgaron una URI Ãºnica para poder ser vinculada a otros datos. Esto provocÃ³ la conexiÃ³n de millones de datos de forma abierta, permitiendo que otros proyectos tambiÃ©n se conectaran a esos datos, y asÃ­ formando lo que se conoce como LOD Cloud, un cloud de datasets (conjunto de datos) abiertos interconectados mediante URIs y accesibles mediante protocolos estÃ¡ndar.

La adopciÃ³n masiva de estÃ¡ndares abiertos como URI, URL, HTTP, HTML, RDF, RDF-Turtle y el lenguaje de consulta SPARQL, termina afianzando la idea de Linked Open Data con miles de grafos de conocimiento conectados.


Modelos de Lenguajes

El objetivo es predecir cuÃ¡l es la palabra que viene a continuaciÃ³n. MÃ¡s formalmente, dada una secuencia de palabras x(1), x(2), â€¦, x(t), determinar la distribuciÃ³n de probabilidad de la siguiente palabra x(t+1): 


Un sistema que lleva a cabo Ã©sto es un modelo de lenguaje.


Tipos generales de modelos

Modelos discriminativos

Tratan de aprender patrones que permiten diferenciar objetos de una clase de los de otras clases. El Ã©nfasis se pone en encontrar los lÃ­mites de decisiÃ³n que determinan cuÃ¡ndo un objeto deberÃ­a ser asignado a una u otra clase. Modelos usuales del aprendizaje supervisado clÃ¡sico usado en problemas de clasificaciÃ³n.

Un modelo discriminativo aprende directamente el lÃ­mite de decisiÃ³n entre clases, es decir que, dadas entradas de ejemplo, un modelo discriminativo generarÃ¡ una categorÃ­a de salida, pero no se puede usar un modelo de estos para generar palabras aleatorias que sean representativas de una categorÃ­a. La regresiÃ³n logÃ­stica, los Ã¡rboles de decisiÃ³n y las mÃ¡quinas de vectores son modelos discriminativos.

Los modelos discriminativos ponen su Ã©nfasis en definir el lÃ­mite de decisiÃ³n, es decir, en realizar la tarea de clasificaciÃ³n que se les pidiÃ³, tienden a tener un mejor rendimiento en el lÃ­mite, con una cantidad arbitraria de datos de entrenamiento (si puede ser mayor, mejor, y pueden superar a los generativos, ya que estos funcionan mejor con pocos datos que los discriminativos).



Modelos generativos

Aprenden modelos (probabilÃ­sticos) que describen cÃ³mo los datos de entrenamiento fueron generados. Estos modelos permiten caracterizar los lÃ­mites de las clases y generar (por muestreo) nuevas instancias de las clases. En ese sentido, un generador es lo inverso a un clasificador.

Un modelo generativo modela la distribuciÃ³n de probabilidad de cada clase. Dado un modelo separado para cada posible categorÃ­a de texto, donde incluye la probabilidad previa de la categorÃ­a (P(CategorÃ­a=Clima), ejemplo), asÃ­ como la probabilidad condicional (P(Entradas|CategorÃ­a=Clima)), el modelo puede calcular la probabilidad conjunta (P(Entradas,CategorÃ­a=Clima)) y podemos generar una selecciÃ³n aleatoria de palabras que sea representativa de los textos de la categorÃ­a (Clima).

Los modelos generativos funcionan con una cantidad limitada de datos, pero especÃ­fica, donde primero contienen los Datos de entrenamiento, luego por entrenamiento el modelo generativo aplica ruido aleatorio, y realiza un muestreo generativo, ejemplo:



Para crear un modelo generativo necesitamos muchos datos, pero para ajustar un modelo generativo existente, podemos usar pocos datos muy especÃ­ficos. Para ajustar un modelo existente, realizamos tÃ©cnicas como fine-tuning o few-shot learning, donde podemos usar pocos datos muy especÃ­ficos pero con calidad, o a veces con cientos o miles de ejemplos bien elegidos.
Importante: en un modelo generativo auto-regresivo, se puede generar texto por muestreo repetido. La salida de un muestreo, es la entrada del prÃ³ximo paso.


La explosiÃ³n de la IA generativa

Variational Autoencoders (VAEs)
Generative Adversarial Networks (GANs)
Difussion models 
Transformers 
Neural Radiance Fields (NeRFs)


Transformers




CaracterÃ­sticas distintivas de ChatGPT

Permite interactuar con los usuarios en una forma conversacional (mediante una interfaz de diÃ¡logo). Para ello, es pre-entrenado con un corpus ajustado a las
instrucciones (usado en InstructGPT) convertido en formato conversacional. TambiÃ©n usa un enfoque RLHF (Reinforcement Learning with Human Feedback) que le permite alinearse mÃ¡s adecuadamente con las preferencias humanas a la hora de generar texto.

Agentes inteligentes basados en LLMs


Algunas limitaciones de los LLMs son:

Conocimiento implÃ­cito.
Alucinaciones.
Son estocÃ¡sticos/probabilÃ­sticos.
Caja negra.
Carencia de datos especÃ­ficos del dominio / nuevo conocimiento.
Son muy grandes.
No tienen memoria.

Algunas ventajas de los LLMs:

Conocimiento general.
Procesamiento del lenguaje.
GeneralizaciÃ³n.

Algunas desventajas de los grafos de conocimiento (KGs):

Incompletitud.
Falta de comprensiÃ³n de lenguaje.
Hechos invisibles.

Algunas ventajas de los grafos de conocimiento (KGs):

Conocimiento estructurado.
PrecisiÃ³n.
DecisiÃ³n.
Interpretabilidad.
Conocimiento especÃ­fico del dominio.
Conocimiento en evoluciÃ³n.


IntegraciÃ³n de los LLMs y los KGs



Esta integraciÃ³n basa su aprendizaje por contexto, donde se utiliza la GeneraciÃ³n Aumentada por la RecuperaciÃ³n (Retrieval Augmented Generation (RAG)). 

Aprendizaje en contexto (ICL)

El objetivo es aprender por analogÃ­as, es decir que se basa en un aprendizaje efÃ­mero: el agente razona sobre los ejemplos que tiene a mano, sin cambiar su memoria permanente. Es lo contrario a modificar explÃ­citamente la representaciÃ³n interna del agente.
In-Context Learning es un modelo que aprende a partir del contexto proporcionado en la entrada, sin necesidad de ajustar sus parÃ¡metros internos.

Los pasos en la decisiÃ³n de un LLM con ICL son:
ICL requiere algunos ejemplos para formar un contexto de demostraciÃ³n.
ICL concatena una pregunta de consulta y el contexto de demostraciÃ³n para formar un mensaje/comando (prompt).
El prompt es introducido en el modelo de lenguaje para su predicciÃ³n.

Difiere del aprendizaje supervisado ya que no hay actualizaciones de parÃ¡metros, predice directamente con el modelo pre-entrenado y el contexto provisto.


RAG



Una aplicaciÃ³n RAG tÃ­pica tiene dos componentes principales: 
IndexaciÃ³n: un â€œpipelineâ€ para ingerir datos de una fuente e indexarlos. Esto suele realizarse â€œoff-lineâ€.
Load (cargar el documento). AquÃ­ se ingieren documentos de distintas fuentes.
Split (Separar el documento). Se dividen los documentos en chunks o trozos mÃ¡s pequeÃ±os. Mejora la precisiÃ³n al recuperar partes relevantes en lugar de documentos enteros.
Realiza vectores embeddings. Cada fragmento se transforma en un vector embedding usando un modelo de lenguaje pre entrenado. Captura el significado semÃ¡ntico.
Store (Guardar vectores y embeddings). Los vectores se guardan en un vector database, y tambiÃ©n se almacenan los metadatos asociados.
RecuperaciÃ³n y generaciÃ³n: la cadena RAG real, que toma la consulta del usuario en tiempo de ejecuciÃ³n y recupera los datos relevantes del Ã­ndice, luego los pasa al modelo:
Question.
Retrieve. El sistema convierte la pregunta en un vector de embeddings y se realiza una bÃºsqueda vectorial en el Ã­ndice para encontrar los fragmentos de textos mÃ¡s relevantes.
Prompt. AquÃ­ estÃ¡ la pregunta original y los fragmentos recuperados.
Generate (ChatModel/LLM).
Answer.



TeorÃ­a 9 - Procesos de decisiones Markov (MDP)


La IA ha considerado tradicionalmente que en el proceso de selecciÃ³n de acciones, cumple un rol fundamental el concepto de bÃºsqueda y la utilizaciÃ³n de conocimiento especÃ­fico del dominio. Como ejemplo, en la resoluciÃ³n de problemas mediante bÃºsqueda heurÃ­stica y problemas de planificaciÃ³n (planning) clÃ¡sico. Estas tÃ©cnicas producen una soluciÃ³n a un problema que consiste en una secuencia de acciones que permite conducir el sistema desde un estado inicial a uno de los estados objetivos.

Este esquema tradicional para la resoluciÃ³n de problemas se basa en el supuesto de que las acciones son determinÃ­sticas, que se puede definir como:

f : S Ã— A â†’ S

Conociendo la funciÃ³n de transiciÃ³n y los costos asociados con las acciones, es posible obtener una soluciÃ³n (en algunos casos Ã³ptima) observando simplemente el estado inicial y posteriormente armando un plan que conduzca a un estado objetivo, garantizando que siempre se llega a ese estado (por ser determinÃ­stico). Por ejemplo 8-puzzle, cubo de Rubik, etc.

Existen situaciones sin embargo, donde el agente no tiene certeza del efecto de sus acciones. Por ejemplo en las acciones de un robot real, al existir componentes mecÃ¡nicas involucrados, existe la posibilidad de que una acciÃ³n determinada no se ejecute correctamente. 
En estos casos, la incertidumbre sobre el efecto de las acciones suele implicar la necesidad de combinar la observaciÃ³n del ambiente con la ejecuciÃ³n de las acciones, y constituyen el tipo de problemas que pueden ser referenciados como problemas de contingencia.

Al hablar de incertidumbre en las acciones, no estamos queriendo significar que el agente no tenga ningÃºn tipo de informaciÃ³n sobre sus efectos, sino simplemente resaltar que no son determinÃ­sticas en cuanto al estado resultante. 


